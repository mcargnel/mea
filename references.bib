#  Classic econometrics

@book{Cameron_Trivedi_2005,
	place={Cambridge},
	title={Microeconometrics: Methods and Applications},
	publisher={Cambridge University Press},
    author={Cameron, A. Colin and Trivedi, Pravin K.},
	year={2005}}



#  Machine Learning

@book{CART,
	author = {Leo Breiman and Jerome H. Friedman and Richard A. Olshen and Charles J. Stone},
	title = {Classification and Regression Trees},
	year = {1984},
	publisher = {Routledge},
	doi = {10.1201/9781315139470}
	
}

@book{ISLP,
	title = {An Introduction to Statistical Learning: with Applications in R},
	author = {Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani},
	year = {2023},
	publisher = {Springer},
	doi = {10.1007/978-3-031-38747-0}
}

@book{ESL,
	title = {The Elements of Statistical Learning},
	author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	year = {2009},
	publisher = {Springer},
	doi = {10.1007/978-0-387-84858-7}
}

@article{breiman2001a,
	author       = {Leo Breiman},
	title        = {Random Forests},
	journal      = {Machine Learning},
	year         = {2001},
	volume       = {45},
	number       = {1},
	pages        = {5--32},
	doi          = {10.1023/A:1010933404324},
	url          = {https://doi.org/10.1023/A:1010933404324},
	abstract     = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	issn         = {1573-0565}
}


# Machine learning and economics 

@TechReport{desai_2023,
  author={Ajit Desai},
  title={{Machine Learning for Economics Research: When What and How?}},
  year=2023,
  month=Mar,
  institution={arXiv.org},
  type={Papers},
  url={https://ideas.repec.org/p/arx/papers/2304.00086.html},
  number={2304.00086},
  abstract={This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used to process nontraditional and unstructured data, capture strong nonlinearity, and improve prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggests that ML is becoming an essential addition to the econometrician's toolbox.},
  keywords={},
  doi={},
}

@article{varian_2014,
Author = {Varian, Hal R.},
Title = {Big Data: New Tricks for Econometrics},
Journal = {Journal of Economic Perspectives},
Volume = {28},
Number = {2},
Year = {2014},
Month = {May},
Pages = {3–28},
DOI = {10.1257/jep.28.2.3},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3}}

@article{mullainathan_2017,
Author = {Mullainathan, Sendhil and Spiess, Jann},
Title = {Machine Learning: An Applied Econometric Approach},
Journal = {Journal of Economic Perspectives},
Volume = {31},
Number = {2},
Year = {2017},
Month = {May},
Pages = {87–106},
DOI = {10.1257/jep.31.2.87},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87}}

@TechReport{athey_2015,
  author={Athey, Susan and Imbens, Guido W.},
  title={{Machine Learning for Estimating Heterogeneous Causal Effects}},
  year=2015,
  month=Apr,
  institution={Stanford University, Graduate School of Business},
  type={Research Papers},
  url={https://ideas.repec.org/p/ecl/stabus/3350.html},
  number={3350},
  abstract={In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the \&quot;ground truth\&quot; for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.},
  keywords={},
  doi={},
}

@misc{wager_2017,
      title={Estimation and Inference of Heterogeneous Treatment Effects using Random Forests}, 
      author={Stefan Wager and Susan Athey},
      year={2017},
      eprint={1510.04342},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1510.04342}, 
}


@article{Grimmer_Messing_Westwood_2017,
      title={Estimating Heterogeneous Treatment Effects and the Effects of Heterogeneous Treatments with Ensemble Methods},
      volume={25},
      DOI={10.1017/pan.2017.15},
      number={4},
      journal={Political Analysis},
      author={Grimmer, Justin and Messing, Solomon and Westwood, Sean J.},
      year={2017},
      pages={413–434}}

@misc{athey_2018,
      title={Generalized Random Forests}, 
      author={Susan Athey and Julie Tibshirani and Stefan Wager},
      year={2018},
      eprint={1610.01271},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1610.01271}, 
}


# Interpretable Machine Learning

@book{molnar2025,
  title={Interpretable Machine Learning},
  subtitle={A Guide for Making Black Box Models Explainable},
  author={Christoph Molnar},
  year={2025},
  edition={3},
  isbn={978-3-911578-03-5},
  url={https://christophm.github.io/interpretable-ml-book}
}

@article{breiman_2001,
author = {Leo Breiman},
title = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
volume = {16},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {199 -- 231},
year = {2001},
doi = {10.1214/ss/1009213726},
URL = {https://doi.org/10.1214/ss/1009213726}
}

@article{to_explain_or_to_predict,
	author = {Galit Shmueli},
	title = {{To Explain or to Predict?}},
	volume = {25},
	journal = {Statistical Science},
	number = {3},
	publisher = {Institute of Mathematical Statistics},
	pages = {289 -- 310},
	keywords = {causality, data mining, Explanatory modeling, predictive modeling, predictive power, scientific research, statistical strategy},
	year = {2010},
	doi = {10.1214/10-STS330},
	URL = {https://doi.org/10.1214/10-STS330}
}

@article{Zhao2021,
author = {Qingyuan Zhao and Trevor Hastie and},
title = {Causal Interpretations of Black-Box Models},
journal = {Journal of Business \& Economic Statistics},
volume = {39},
number = {1},
pages = {272--281},
year = {2021},
publisher = {ASA Website},
doi = {10.1080/07350015.2019.1624293},
URL = { 
        https://doi.org/10.1080/07350015.2019.1624293
		},
eprint = { 
        https://doi.org/10.1080/07350015.2019.1624293
		}
}

@inbook{Molnar_2023,
   title={Relating the Partial Dependence Plot and Permutation Feature Importance to the Data Generating Process},
   ISBN={9783031440649},
   ISSN={1865-0937},
   url={http://dx.doi.org/10.1007/978-3-031-44064-9_24},
   DOI={10.1007/978-3-031-44064-9_24},
   booktitle={Explainable Artificial Intelligence},
   publisher={Springer Nature Switzerland},
   author={Molnar, Christoph and Freiesleben, Timo and König, Gunnar and Herbinger, Julia and Reisinger, Tim and Casalicchio, Giuseppe and Wright, Marvin N. and Bischl, Bernd},
   year={2023},
   pages={456–479} 
}

@book{freiesleben2024supervised,
  title    = {Supervised Machine Learning for Science},
  author   = {Timo Freiesleben and Christoph Molnar},
  year     = {2024},
  subtitle = {How to stop worrying and love your black box},
  url      = {https://ml-science-book.com/}
}

@article{Goldstein02015,
author = {Alex Goldstein and Adam Kapelner and Justin Bleich and Emil Pitkin and},
title = {Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
journal = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {1},
pages = {44--65},
year = {2015},
publisher = {ASA Website},
doi = {10.1080/10618600.2014.907095},
URL = { 
        https://doi.org/10.1080/10618600.2014.907095
},
eprint = { 
        https://doi.org/10.1080/10618600.2014.907095
}
}

@misc{fisher2019,
      title={All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously}, 
      author={Aaron Fisher and Cynthia Rudin and Francesca Dominici},
      year={2019},
      eprint={1801.01489},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1801.01489}, 
}

@article{friedman_2001,
author = {Jerome H. Friedman},
title = {{Greedy function approximation: A gradient boosting machine.}},
volume = {29},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1189 -- 1232},
keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
year = {2001},
doi = {10.1214/aos/1013203451},
URL = {https://doi.org/10.1214/aos/1013203451}
}

# Causal Machine Learning

@article{Chernozhukov_2018,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = {Double/debiased machine learning for treatment and structural parameters},
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = {We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}


# Difference in difference
@Article{callway_santana_2021,
journal={Journal of Econometrics},
author={Callaway, Brantly and Santa'Anna, Pedro H.C.},
title={Difference-in-Differences with multiple time periods},
year={2021},
month={None},
pages={200-230},
volume={225},
number={2},
abstract={In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the â€œparallel trends assumptionâ€ holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001â€“2007. Open-source software is available for implementing the proposed methods.},
keywords={Difference-in-Differences; Dynamic treatment effects; Doubly robust; Event study; Variation in treat},
doi={10.1016/j.jeconom.2020.12.001},
url={https://ideas.repec.org/a/eee/econom/v225y2021i2p200-230.html},
}

@Article{bacon_2021,
journal={Journal of Econometrics},
author={Goodman-Bacon, Andrew},
title={Difference-in-differences with variation in treatment timing},
year={2021},
month={None},
pages={254-277},
volume={225},
number={2},
abstract={The canonical difference-in-differences (DD) estimator contains two time periods, â€preâ€ and â€postâ€, and two groups, â€treatmentâ€ and â€controlâ€. Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of two-way fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.},
keywords={Difference-in-differences; Variation in treatment timing; Two-way fixed effects; Treatment effect he},
doi={10.1016/j.jeconom.2021.03.014},
url={https://ideas.repec.org/a/eee/econom/v225y2021i2p254-277.html},
}

@book{Cunningham_2021,
 ISBN = {9780300251685},
 URL = {http://www.jstor.org/stable/j.ctv1c29t27},
 abstract = {
An accessible and contemporary introduction to the methods
for determining cause and effect in the social sciences
Causal inference encompasses the tools that allow social scientists
to determine what causes what. Economists-who generally can't run
controlled experiments to test and validate their hypotheses-apply
these tools to observational data to make connections. In a messy
world, causal inference is what helps establish the causes and
effects of the actions being studied, whether the impact (or lack
thereof) of increases in the minimum wage on employment, the
effects of early childhood education on incarceration later in
life, or the introduction of malaria nets in developing regions on
economic growth. Scott Cunningham introduces students and
practitioners to the methods necessary to arrive at meaningful
answers to the questions of causation, using a range of modeling
techniques and coding instructions for both the R and Stata
programming languages.
},
 author = {Scott Cunningham},
 publisher = {Yale University Press},
 title = {Causal Inference: The Mixtape},
 urldate = {2025-10-14},
 year = {2021}
}

@Article{sun_abraha_sarah_2021,
journal={Journal of Econometrics},
author={Sun, Liyang and Abraham, Sarah},
title={Estimating dynamic treatment effects in event studies with heterogeneous treatment effects},
year={2021},
month={None},
pages={175-199},
volume={225},
number={2},
abstract={To estimate the dynamic effects of an absorbing treatment, researchers often use two-way fixed effects regressions that include leads and lags of the treatment. We show that in settings with variation in treatment timing across units, the coefficient on a given lead or lag can be contaminated by effects from other periods, and apparent pretrends can arise solely from treatment effects heterogeneity. We propose an alternative estimator that is free of contamination, and illustrate the relative shortcomings of two-way fixed effects regressions with leads and lags through an empirical application.},
keywords={Difference-in-differences; Two-way fixed effects; Pretrend test},
doi={10.1016/j.jeconom.2020.09.006},
url={https://ideas.repec.org/a/eee/econom/v225y2021i2p175-199.html},
}



@article{chang_2020,
    author = {Chang, Neng-Chieh},
    title = {Double/debiased machine learning for difference-in-differences models},
    journal = {The Econometrics Journal},
    volume = {23},
    number = {2},
    pages = {177-191},
    year = {2020},
    month = {02},
    abstract = {This paper provides an orthogonal extension of the semiparametric difference-in-differences estimator proposed in earlier literature. The proposed estimator enjoys the so-called Neyman orthogonality (Chernozhukov et al., 2018), and thus it allows researchers to flexibly use a rich set of machine learning methods in the first-step estimation. It is particularly useful when researchers confront a high-dimensional data set in which the number of potential control variables is larger than the sample size and the conventional nonparametric estimation methods, such as kernel and sieve estimators, do not apply. I apply this orthogonal difference-in-differences estimator to evaluate the effect of tariff reduction on corruption. The empirical results show that tariff reduction decreases corruption in large magnitude.},
    issn = {1368-4221},
    doi = {10.1093/ectj/utaa001},
    url = {https://doi.org/10.1093/ectj/utaa001},
    eprint = {https://academic.oup.com/ectj/article-pdf/23/2/177/37933844/utaa001.pdf},
}


@article{DoubleML2022,
      title   = {{DoubleML} -- {A}n Object-Oriented Implementation of Double Machine Learning in {P}ython},
      author  = {Philipp Bach and Victor Chernozhukov and Malte S. Kurz and Martin Spindler},
      journal = {Journal of Machine Learning Research},
      year    = {2022},
      volume  = {23},
      number  = {53},
      pages   = {1--6},
      url     = {http://jmlr.org/papers/v23/21-0862.html}
}

@inproceedings{lightgbm_2017,
author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
title = {LightGBM: a highly efficient gradient boosting decision tree},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {3149–3157},
numpages = {9},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{seabold2010statsmodels,
  title={statsmodels: Econometric and statistical modeling with python},
  author={Seabold, Skipper and Perktold, Josef},
  booktitle={9th Python in Science Conference},
  year={2010},
}

# Difference in difference
@Article{gonzales_2025,
journal={Public Choice},
author={Juan Pablo GonzÃ¡lez},
title={Environmental regulation, regulatory spillovers and rent-seeking},
year={2025},
month={January},
pages={217-250},
volume={202},
number={1},
abstract={ How do special interests react to an increase in their regulatory burden? In this paper, I use a shock to the regulatory environment by analyzing state-level enforcement of the Clean Air Act during the fracking boom. First, I show that fracking is associated with an increase in state regulatory activities for non-energy-related industries, generating regulatory spillovers to firms unrelated to fracking. Using the fact that fracking had regulatory spillovers to other industries, I use the presence of fracking as an instrument for environmental regulation for non-energy-related firms. I find that increased environmental enforcement is associated with an increase in state campaign contributions going to Republicans, and particularly to legislative races in competitive districts. These results provide some of the first evidence that changes in the regulatory environment can spur private sector mobilization with the potential to affect broader areas of policy through its electoral consequences.},
keywords={Environmental regulation; Campaign contributions; Fracking; Rent-seeking},
doi={10.1007/s11127-024-01189-7},
url={https://ideas.repec.org/a/kap/pubcho/v202y2025i1d10.1007_s11127-024-01189-7.html},
}

