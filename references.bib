#  Panel data analysis and classical econometrics

@book{Cameron_Trivedi_2005,
	place={Cambridge},
	title={Microeconometrics: Methods and Applications},
	publisher={Cambridge University Press},
    author={Cameron, A. Colin and Trivedi, Pravin K.},
	year={2005}}

@Book{Wooldridge_2010,
  author={Jeffrey M Wooldridge},
  title={{Econometric Analysis of Cross Section and Panel Data}},
  publisher={The MIT Press},
  year=2010,
  month={December},
  volume={1},
  number={0262232588},
  series={MIT Press Books},
  edition={},
  keywords={econometrics; cross section data; panel data},
  doi={},
  isbn={ARRAY(0x843ae838)},
  abstract={The second edition of this acclaimed graduate text provides a unified treatment of the analysis of two kinds of data structures used in contemporary econometric research: cross section data and panel data. The book covers both linear and nonlinear models, including models with dynamics and/or individual heterogeneity. In addition to general estimation frameworks (particularly methods of moments and maximum likelihood), specific linear and nonlinear methods are covered in detail, including probit and logit models, multinomial and ordered choice models, Tobit models and two-part extensions, models for count data, various censored and missing data schemes, causal (or treatment) effect estimation, and duration analysis. Control function and correlated random effects approaches are expanded to allow estimation of complicated models in the presence of endogeneity and heterogeneity. This second edition has been substantially updated and revised. Improvements include a broader class of models for missing data problems; more detailed treatment of cluster sampling problems, an important topic for empirical researchers; expanded discussion of \&quot;generalized instrumental variables\&quot; (GIV) estimation; new coverage of inverse probability weighting; a more complete framework for estimating treatment effects with assumptions concerning the intervention and different data structures, including panel data, and a firmly established link between econometric approaches to nonlinear panel data and the \&quot;generalized estimating equation\&quot; literature popular in statistics and other fields. New attention is given to explaining when particular econometric methods can be applied; the goal is not only to tell readers what does work, but why certain “obvious” procedures do not. The numerous included exercises, both theoretical and computer-based, allow the reader to extend methods covered in the text and discover new insights.},
  url={https://ideas.repec.org/b/mtp/titles/0262232588.html}
}

#  Machine Learning in economics and panel data
@TechReport{athey_2015,
  author={Athey, Susan and Imbens, Guido W.},
  title={{Machine Learning for Estimating Heterogeneous Causal Effects}},
  year=2015,
  month=Apr,
  institution={Stanford University, Graduate School of Business},
  type={Research Papers},
  url={https://ideas.repec.org/p/ecl/stabus/3350.html},
  number={3350},
  abstract={In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the \&quot;ground truth\&quot; for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.},
  keywords={},
  doi={},
}
@article{mullainathan_2017,
Author = {Mullainathan, Sendhil and Spiess, Jann},
Title = {Machine Learning: An Applied Econometric Approach},
Journal = {Journal of Economic Perspectives},
Volume = {31},
Number = {2},
Year = {2017},
Month = {May},
Pages = {87–106},
DOI = {10.1257/jep.31.2.87},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87}}

@article{varian_2014,
Author = {Varian, Hal R.},
Title = {Big Data: New Tricks for Econometrics},
Journal = {Journal of Economic Perspectives},
Volume = {28},
Number = {2},
Year = {2014},
Month = {May},
Pages = {3–28},
DOI = {10.1257/jep.28.2.3},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3}}

# Random Forests in Panel data
@misc{wager_2017,
      title={Estimation and Inference of Heterogeneous Treatment Effects using Random Forests}, 
      author={Stefan Wager and Susan Athey},
      year={2017},
      eprint={1510.04342},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1510.04342}, 
}

@misc{athey_2018,
      title={Generalized Random Forests}, 
      author={Susan Athey and Julie Tibshirani and Stefan Wager},
      year={2018},
      eprint={1610.01271},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1610.01271}, 
}

@misc{chernozhukov_2024,
      title={Double/Debiased Machine Learning for Treatment and Causal Parameters}, 
      author={Victor Chernozhukov and Denis Chetverikov and Mert Demirer and Esther Duflo and Christian Hansen and Whitney Newey and James Robins},
      year={2024},
      eprint={1608.00060},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1608.00060}, 
}

# Interpretable Machine Learning

@article{Zhao2021,
author = {Qingyuan Zhao and Trevor Hastie and},
title = {Causal Interpretations of Black-Box Models},
journal = {Journal of Business \& Economic Statistics},
volume = {39},
number = {1},
pages = {272--281},
year = {2021},
publisher = {ASA Website},
doi = {10.1080/07350015.2019.1624293},
URL = { 
        https://doi.org/10.1080/07350015.2019.1624293
		},
eprint = { 
        https://doi.org/10.1080/07350015.2019.1624293
		}
}

@inbook{Molnar_2023,
   title={Relating the Partial Dependence Plot and Permutation Feature Importance to the Data Generating Process},
   ISBN={9783031440649},
   ISSN={1865-0937},
   url={http://dx.doi.org/10.1007/978-3-031-44064-9_24},
   DOI={10.1007/978-3-031-44064-9_24},
   booktitle={Explainable Artificial Intelligence},
   publisher={Springer Nature Switzerland},
   author={Molnar, Christoph and Freiesleben, Timo and König, Gunnar and Herbinger, Julia and Reisinger, Tim and Casalicchio, Giuseppe and Wright, Marvin N. and Bischl, Bernd},
   year={2023},
   pages={456–479} 
}

@article{Goldstein02015,
author = {Alex Goldstein and Adam Kapelner and Justin Bleich and Emil Pitkin and},
title = {Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
journal = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {1},
pages = {44--65},
year = {2015},
publisher = {ASA Website},
doi = {10.1080/10618600.2014.907095},
URL = { 
        https://doi.org/10.1080/10618600.2014.907095
},
eprint = { 
        https://doi.org/10.1080/10618600.2014.907095
}
}

@misc{fisher2019,
      title={All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously}, 
      author={Aaron Fisher and Cynthia Rudin and Francesca Dominici},
      year={2019},
      eprint={1801.01489},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1801.01489}, 
}

@article{friedman_2001,
author = {Jerome H. Friedman},
title = {{Greedy function approximation: A gradient boosting machine.}},
volume = {29},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1189 -- 1232},
keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
year = {2001},
doi = {10.1214/aos/1013203451},
URL = {https://doi.org/10.1214/aos/1013203451}
}


@book{CART,
	author = {Leo Breiman and Jerome H. Friedman and Richard A. Olshen and Charles J. Stone},
	title = {Classification and Regression Trees},
	year = {1984},
	publisher = {Routledge},
	doi = {10.1201/9781315139470}
	
}

@book{ISLP,
	title = {An Introduction to Statistical Learning: with Applications in R},
	author = {Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani},
	year = {2023},
	publisher = {Springer},
	doi = {10.1007/978-3-031-38747-0}
}

@book{ESL,
	title = {The Elements of Statistical Learning},
	author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	year = {2009},
	publisher = {Springer},
	doi = {10.1007/978-0-387-84858-7}
}

@article{to_explain_or_to_predict,
	author = {Galit Shmueli},
	title = {{To Explain or to Predict?}},
	volume = {25},
	journal = {Statistical Science},
	number = {3},
	publisher = {Institute of Mathematical Statistics},
	pages = {289 -- 310},
	keywords = {causality, data mining, Explanatory modeling, predictive modeling, predictive power, scientific research, statistical strategy},
	year = {2010},
	doi = {10.1214/10-STS330},
	URL = {https://doi.org/10.1214/10-STS330}
}

@article{breiman2001a,
	author       = {Leo Breiman},
	title        = {Random Forests},
	journal      = {Machine Learning},
	year         = {2001},
	volume       = {45},
	number       = {1},
	pages        = {5--32},
	doi          = {10.1023/A:1010933404324},
	url          = {https://doi.org/10.1023/A:1010933404324},
	abstract     = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	issn         = {1573-0565}
}
