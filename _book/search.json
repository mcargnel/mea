[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Random Forest in Economic Panel Data Analysis: Fundamentals and Methodology",
    "section": "",
    "text": "Summary\nPanel data analysis has historically been approached through classical econometric techniques, such as linear fixed and random effects models. While these methods are widely used for causal inference and control of unobserved heterogeneity, they impose strong functional restrictions and require a priori specification of relationships between variables. In contexts where the data structure is complex and non-linear, these limitations can affect the validity or accuracy of the results. In this framework, machine learning models (particularly tree ensembles like Random Forest) emerge as complementary tools that allow modeling complex relationships without the need to specify rigid functional forms. Although traditionally considered “black box” methods, in the last decade interpretative techniques have emerged that allow extracting useful and comprehensible information for applied researchers. This work aims to contribute to the economic analysis of panel data by introducing a theoretical and practical framework for the use of Random Forest, offering applied economists a flexible and complementary alternative to conventional econometric approaches.\nKeywords: Ensemble of trees, Bagging, Random Forest, Panel Data Analysis, Econometrics.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "sections/1_intro.html",
    "href": "sections/1_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Limitations of Classical Panel Data Models\nPanel data, which consists of observations on multiple individuals (such as people, firms, or countries) over multiple time periods, is widely used in econometrics. The inclusion of a temporal dimension in cross-sectional data offers several advantages, such as greater variability than a purely cross-sectional sample, the ability to control for unobservable heterogeneity between individuals, and the opportunity to analyze dynamic effects, conduct event studies, and evaluate policy impacts (Cameron & Trivedi, 2005). However, despite their widespread use in econometrics, panel data methods share two key limitations with the classic linear model: the treatment of interactions and the assumption of linearity.\nThe reason of the limitations rely on the underlying model behind the estimation. If we think about the specification of a linear fixed or random effects models, we can see the following\nThe linear fixed effects model can be written as:\n\\[\ny_{it} = \\alpha_i + \\mathbf{x}_{it}'\\boldsymbol{\\beta} + \\varepsilon_{it}\n\\]\nwhere \\(y_{it}\\) is the outcome variable for individual \\(i\\) at time \\(t\\), \\(\\alpha_i\\) the individual-specific intercept (captures unobserved heterogeneity that is constant over time for each individual), \\(\\mathbf{x}_{it}\\) the vector of observed explanatory variables for individual \\(i\\) at time \\(t\\), \\(\\boldsymbol{\\beta}\\) the vector of coefficients associated with \\(\\mathbf{x}_{it}\\) and \\(\\varepsilon_{it}\\) the idiosyncratic error term.\nThe linear random effects model is specified as:\n\\[\ny_{it} = \\alpha + \\mathbf{x}_{it}'\\boldsymbol{\\beta} + u_i + \\varepsilon_{it}\n\\]\nwhere: \\(\\alpha\\): overall intercept (common to all individuals), \\(u_i\\): individual-specific random effect (assumed to be uncorrelated with \\(\\mathbf{x}_{it}\\)) and the other terms are as defined above.\nIn both models, the key distinction is how the individual-specific effect is treated: as a fixed parameter to be estimated for each individual (\\(\\alpha_i\\)) in the fixed effects model, or as a random variable (\\(u_i\\)) in the random effects model.\nBoth specifications assume linearity and independence between variables, assumptions that are often unrealistic in practice. A common workaround, as in classical linear regression, is to manually add transformations (to address non-linearity) and interaction terms. Addressing the non-linearity manually can be problematic because it might rely on subjective choices and can lead to model misspecification and lack of interpretability.\nAddressing interactions manually quickly becomes impractical as the number of variables increases, because the number of possible interaction terms grows rapidly. For example, with \\(p\\) variables, the number of possible \\(k\\)-way interaction terms is given by the binomial coefficient:\n\\[\n\\text{Number of $k$-way interactions} = \\binom{p}{k}\n\\]\nThus, the total number of possible terms (main effects and all possible interactions, not just two way) in a fully specified linear model is: \\[\n\\sum_{k=1}^{p} \\binom{p}{k} = 2^p - 1\n\\]\nFor instance, with \\(p=5\\) variables: Main effects (\\(k=1\\)): \\(\\binom{5}{1} = 5\\), Two-way interactions (\\(k=2\\)): \\(\\binom{5}{2} = 10\\), Three-way interactions (\\(k=3\\)): \\(\\binom{5}{3} = 10\\), Four-way interactions (\\(k=4\\)): \\(\\binom{5}{4} = 5\\) and Five-way interaction (\\(k=5\\)): \\(\\binom{5}{5} = 1\\). Leading to a total numbers 31 terms. As \\(p\\) increases, the number of terms grows combinatorially, making it infeasible to specify and interpret all possible interactions in a linear model. A more visual way of this can be seeing this phenomenon can be found in Figure 1.1 where the number of interaction terms grows as a function of the number of variables.\nFigure 1.1: Number of interaction terms, as a function of the number of variables. It can be seen that the number of interaction terms grows exponentially with the number of variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "sections/1_intro.html#machine-learning-as-an-alternative",
    "href": "sections/1_intro.html#machine-learning-as-an-alternative",
    "title": "1  Introduction",
    "section": "1.2 Machine Learning as an Alternative",
    "text": "1.2 Machine Learning as an Alternative\nTo the best of the author’s knowledge, there are currently no practical solutions within the traditional linear modeling framework to fully overcome the challenges of specifying and interpreting all possible non-linearities and interaction effects as the number of variables increases. This motivates the need for a different methodological approach, which is the focus of this document.\nIn this work, a methodology is proposed that combines three key elements: (1) the use of a flexible, non-parametric algorithm: Random Forests; (2) the application of interpretable machine learning techniques; and (3) the adaptation of these tools to account for the time-dependent structure of panel data. Random Forest is an ensemble learning method that aggregates the predictions of multiple decision trees to improve predictive accuracy and robustness. Unlike linear models, Random Forests do not require the analyst to specify the functional form of relationships between variables or to manually enumerate interaction terms. The algorithm naturally captures complex, nonlinear relationships and high-order interactions among features, thereby alleviating the risk of model misspecification due to omitted nonlinearities or interactions.\nHowever, adopting Random Forests in the context of panel data analysis introduces two main challenges. First, the standard Random Forest algorithm assumes that all observations are independent, which is often violated in panel data where repeated measurements are taken from the same individuals over time. Second, machine learning models, including Random Forests, are frequently criticized for their lack of interpretability, as their internal workings are less transparent than those of traditional statistical models. This “black-box” nature can hinder substantive understanding and limit their adoption in applied research.\nThe first challenge—handling time dependence—will be addressed in later sections, where we discuss modifications to the modeling approach that account for the temporal structure of the data.\nTo address the second challenge—interpretability—this document introduces and demonstrates three interpretable machine learning techniques: Permutation Feature Importance (PFI), Individual Conditional Expectation (ICE) plots, and Partial Dependence Plots (PDP). PFI provides a global ranking of the importance of each independent variable by measuring the increase in prediction error when the variable’s values are randomly permuted. ICE plots visualize how the predicted outcome for individual observations changes as a single feature varies, offering insight into heterogeneous effects. PDPs, on the other hand, show the average effect of one or more features on the predicted outcome, helping to reveal general patterns and marginal relationships. Together, these tools enable researchers to move beyond “black-box” predictions and gain a deeper understanding of the relationships captured by the Random Forest model, both at the global and individual level.\nIn summary, this document presents for a modern, flexible, and interpretable approach to panel data analysis that leverages the strengths of machine learning while addressing its traditional limitations in the context of social science research. In this context, it’s important to notice that, although PDPs can have a causal interpretation as we will mention later, the goal of this document is not to replace the current methodologies but to provide an additional tool for applied social scientists.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "sections/1_intro.html#document-structure",
    "href": "sections/1_intro.html#document-structure",
    "title": "1  Introduction",
    "section": "1.3 Document Structure",
    "text": "1.3 Document Structure\nThe remainder of this document is organized as follows. Section 2 reviews the existing literature on panel data analysis and the application of machine learning methods, with a particular emphasis on Random Forests and their use in economics and the social sciences. Sections 3, 4, and 5 will be mostly a summary of the methodology and will assume some familiarity with the concepts, Section 3 introduces the structure and key characteristics of panel data, discusses traditional modeling approaches, and highlights the challenges associated with high-dimensional and time-dependent data. Section 4 examines the limitations of conventional linear models in capturing complex relationships and interactions, thereby motivating the need for more flexible methodologies. Section 5 provides an overview of the Random Forest algorithm, outlining its advantages and potential for modeling panel data, as well as discussing its strengths and weaknesses in this context. Section 6 details methodological adaptations and strategies for applying Random Forests to panel data, addressing issues such as time dependence and repeated measurements. Section 7 presents practical examples and empirical applications to illustrate the proposed methodology, including the use of interpretable machine learning techniques. Finally, Section 8 concludes with a summary of key findings, implications for applied research, and suggestions for future work.\n\n\n\n\nCameron, A. C., & Trivedi, P. K. (2005). Microeconometrics: Methods and applications. Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "sections/2_related_work.html",
    "href": "sections/2_related_work.html",
    "title": "2  Related Work",
    "section": "",
    "text": "2.1 Panel Data Analysis in Economics\nThis section overviews the current work in the topics of Panel Data Analysis and Classical Econometrics Methods and Machine Learning in Economics and Panel Data. We will also include the main sources for this study and the contributions that this document offers.\nThe analysis of panel data has been a widely used in empirical economics research and there are many papers and textbooks documenting the methodology. Classical approaches, as documented by (Cameron & Trivedi, 2005) and (Wooldridge, 2010), have primarily relied on linear fixed and random effects models. These methods have proven valuable for controlling unobserved heterogeneity and conducting causal inference. However, these traditional approaches often struggle with complex, non-linear relationships and high-dimensional interactions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "sections/2_related_work.html#machine-learning-in-economics",
    "href": "sections/2_related_work.html#machine-learning-in-economics",
    "title": "2  Related Work",
    "section": "2.2 Machine Learning in Economics",
    "text": "2.2 Machine Learning in Economics\nComprehensive overviews of classic machine learning algorithms—including regularization techniques, tree-based ensembles, and neural networks—are provided in (Hastie, Tibshirani, & Friedman, 2009), while (James, Witten, Hastie, & Tibshirani, 2023) offers a more introductory perspective. The foundational details of Classification and Regression Trees (CART) are discussed in (Breiman, Friedman, Olshen, & Stone, 1984), and the Random Forest algorithm was specifically introduced by (Breiman, 2001).\nIn recent years, (Mullainathan & Spiess, 2017) have explored the practical applications of machine learning in econometrics, particularly emphasizing prediction and cautioning against drawing causal conclusions about the effects of independent variables without careful consideration. (Varian, 2014) highlights how big data and machine learning techniques, especially tree-based models, can complement traditional econometric methods, excelling in settings with non-linearities and complex interactions.\nThe integration of machine learning methods into economics has accelerated, with increasing attention on adapting these tools for econometric analysis. For example, (Athey & Imbens, 2015) provide a framework for using machine learning algorithms in causal inference; (Wager & Athey, 2017) develop methods for estimating heterogeneous treatment effects using Random Forests; (Grimmer, Messing, & Westwood, 2017) extend these approaches to ensemble methods; and (Athey, Tibshirani, & Wager, 2018) introduce Generalized Random Forests for causal inference.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "sections/2_related_work.html#random-forest-in-panel-data",
    "href": "sections/2_related_work.html#random-forest-in-panel-data",
    "title": "2  Related Work",
    "section": "2.3 Random Forest in Panel Data",
    "text": "2.3 Random Forest in Panel Data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "sections/2_related_work.html#interpretability-in-machine-learning",
    "href": "sections/2_related_work.html#interpretability-in-machine-learning",
    "title": "2  Related Work",
    "section": "2.4 Interpretability in Machine Learning",
    "text": "2.4 Interpretability in Machine Learning\nThe challenge of interpretability in machine learning models has been addressed through various approaches and have lead to multiple discussions through the years. One of the seminar works that discuss this is the",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "sections/2_related_work.html#main-contributions",
    "href": "sections/2_related_work.html#main-contributions",
    "title": "2  Related Work",
    "section": "2.5 Main Contributions",
    "text": "2.5 Main Contributions\n\n\n\n\nAthey, S., & Imbens, G. W. (2015). Machine Learning for Estimating Heterogeneous Causal Effects (Research Papers No. 3350). Stanford University, Graduate School of Business. Retrieved from Stanford University, Graduate School of Business website: https://ideas.repec.org/p/ecl/stabus/3350.html\n\n\nAthey, S., Tibshirani, J., & Wager, S. (2018). Generalized random forests. Retrieved from https://arxiv.org/abs/1610.01271\n\n\nBreiman, L. (2001). Random forests. Machine Learning, 45(1), 5–32. https://doi.org/10.1023/A:1010933404324\n\n\nBreiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and regression trees. Routledge. https://doi.org/10.1201/9781315139470\n\n\nCameron, A. C., & Trivedi, P. K. (2005). Microeconometrics: Methods and applications. Cambridge University Press.\n\n\nGrimmer, J., Messing, S., & Westwood, S. J. (2017). Estimating heterogeneous treatment effects and the effects of heterogeneous treatments with ensemble methods. Political Analysis, 25(4), 413–434. https://doi.org/10.1017/pan.2017.15\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning. Springer. https://doi.org/10.1007/978-0-387-84858-7\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An introduction to statistical learning: With applications in r. Springer. https://doi.org/10.1007/978-3-031-38747-0\n\n\nMullainathan, S., & Spiess, J. (2017). Machine learning: An applied econometric approach. Journal of Economic Perspectives, 31(2), 87–106. https://doi.org/10.1257/jep.31.2.87\n\n\nVarian, H. R. (2014). Big data: New tricks for econometrics. Journal of Economic Perspectives, 28(2), 3–28. https://doi.org/10.1257/jep.28.2.3\n\n\nWager, S., & Athey, S. (2017). Estimation and inference of heterogeneous treatment effects using random forests. Retrieved from https://arxiv.org/abs/1510.04342\n\n\nWooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data (Vol. 1). The MIT Press. Retrieved from https://ideas.repec.org/b/mtp/titles/0262232588.html",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "sections/references.html",
    "href": "sections/references.html",
    "title": "References",
    "section": "",
    "text": "Athey, S., & Imbens, G. W. (2015). Machine\nLearning for Estimating Heterogeneous Causal Effects\n(Research Papers No. 3350). Stanford University, Graduate School of\nBusiness. Retrieved from Stanford University, Graduate School of\nBusiness website: https://ideas.repec.org/p/ecl/stabus/3350.html\n\n\nAthey, S., Tibshirani, J., & Wager, S. (2018). Generalized\nrandom forests. Retrieved from https://arxiv.org/abs/1610.01271\n\n\nBreiman, L. (2001). Random forests. Machine Learning,\n45(1), 5–32. https://doi.org/10.1023/A:1010933404324\n\n\nBreiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984).\nClassification and regression trees. Routledge. https://doi.org/10.1201/9781315139470\n\n\nCameron, A. C., & Trivedi, P. K. (2005). Microeconometrics:\nMethods and applications. Cambridge University Press.\n\n\nGrimmer, J., Messing, S., & Westwood, S. J. (2017). Estimating\nheterogeneous treatment effects and the effects of heterogeneous\ntreatments with ensemble methods. Political Analysis,\n25(4), 413–434. https://doi.org/10.1017/pan.2017.15\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning. Springer. https://doi.org/10.1007/978-0-387-84858-7\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An\nintroduction to statistical learning: With applications in r.\nSpringer. https://doi.org/10.1007/978-3-031-38747-0\n\n\nMullainathan, S., & Spiess, J. (2017). Machine learning: An applied\neconometric approach. Journal of Economic Perspectives,\n31(2), 87–106. https://doi.org/10.1257/jep.31.2.87\n\n\nVarian, H. R. (2014). Big data: New tricks for econometrics. Journal\nof Economic Perspectives, 28(2), 3–28. https://doi.org/10.1257/jep.28.2.3\n\n\nWager, S., & Athey, S. (2017). Estimation and inference of\nheterogeneous treatment effects using random forests. Retrieved\nfrom https://arxiv.org/abs/1510.04342\n\n\nWooldridge, J. M. (2010). Econometric Analysis\nof Cross Section and Panel Data (Vol. 1). The MIT Press.\nRetrieved from https://ideas.repec.org/b/mtp/titles/0262232588.html",
    "crumbs": [
      "References"
    ]
  }
]